# Voice-Face Association Learning Evaluation
- Reproduce bunches of  works based on unified standards  ðŸ˜ƒ
- High-speed  training and testing âš¡
- Easy to extend ðŸ’­



## Installation 

1. Clone\download this repo

2. Install requirements

```
pytorch>=1.8.1
wandb>=0.12.10
```

3. Download dataset 

Download `dataset.zip` from [GoogleDrive](https://drive.google.com/file/d/1sVQ7I4_9rwWF18vk4VZFVAx-8Inv-wlT/view?usp=sharing) (2.3GB) and unzip it to the project root.  The folder structure is shown below:

```
dataset
â”œâ”€â”€ evals
â”‚Â Â  â”œâ”€â”€ test_matching_10.pkl
â”‚Â Â  â”œâ”€â”€ test_matching_g.pkl
â”‚Â Â  â”œâ”€â”€ test_matching.pkl
â”‚Â Â  â”œâ”€â”€ test_retrieval.pkl
â”‚Â Â  â”œâ”€â”€ test_verification_g.pkl
â”‚Â Â  â”œâ”€â”€ test_verification.pkl
â”‚Â Â  â””â”€â”€ valid_verification.pkl
â”œâ”€â”€ info
â”‚Â Â  â”œâ”€â”€ name2gender.pkl
â”‚Â Â  â”œâ”€â”€ name2jpgs_wavs.pkl
â”‚Â Â  â”œâ”€â”€ name2movies.pkl
â”‚Â Â  â”œâ”€â”€ name2voice_id.pkl
â”‚Â Â  â”œâ”€â”€ train_valid_test_names.pkl
â”‚Â Â  â””â”€â”€ works
â”‚Â Â      â””â”€â”€ wen_weights.txt
â”œâ”€â”€ face_input.pkl
â””â”€â”€ voice_input.pkl
```



Dataset Description

> The dataset is based on VoxCeleb and is  divided according to "Learnable pins: Crossmodal embeddings for person identity,2018,ECCV"   ( 901/100/250 for train/valid/test )
>
> The face images and voice clips are extracted as vector representations in advance to improve the training speed (`face_input.pkl,voice_input.pkl`).  These features are released by "Self-Lifting: A Novel Framework for Unsupervised Voice-Face Association Learning,ICMR,2022".
> Since the dataset split in Self-Lifting (924/112/189) differs from this project,  the results are not comparable.

## Run a production

- Learnable pins: Crossmodal embeddings for person identity,2018,ECCV

â€‹	``python works/1_pins.py``



- Face-voice matching using cross-modal embeddings,MM,2018

â€‹	`python works/2_FV-CME.py`



- On learning associations of faces and voices,ACCV,2018,

â€‹	`python works/3_LAFV.py`



- Disjoint mapping network for cross-modal matching of voices and faces,ICLR,2019

â€‹	`python works/11_SS_DIM_VFMR_Barlow.py --name=DIMNet `  



- Voice-Face Cross-modal Matching and Retrieval- A Benchmark,2019

â€‹	`python works/11_SS_DIM_VFMR_Barlow.py --name=VFMR `  



- Seeking the Shape of Sound- An Adaptive Framework for Learning Voice-Face Association,CVPR,2021

â€‹	`python works/5_Wen.py`



- Fusion and Orthogonal Projection for Improved Face-Voice Association,ICASSP,2022

â€‹	`python works/6_FOP.py`



- Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast,IJCAI,2022

â€‹	`python works/7_CMPC.py`



- Self-Lifting: A Novel Framework for Unsupervised Voice-Face Association Learning,ICMR,2022

â€‹	`python works/9_SL.py` for self-lifting 

â€‹	`python works/8_CAE.py` for the CCAE baseline 

â€‹	`python works/11_SS_DIM_VFMR_Barlow.py --name=SL-Barlow `  for the Barlow Twins baseline 







## Integration with Wandb

*use [wandb](https://wandb.ai) to view the training process:*

1. Create  `.wb_config.json`  file in the project root, using the following content:

   ```
   {
     "WB_KEY": "Your wandb auth key"
   }
   ```

   

2. add `--dryrun=False` to the training command, for example:   `python main.py --dryrun=False`





## Results

![](./files/result1.png)


